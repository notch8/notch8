<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Notch8</title>
 <link href="http://notch8.com/articles.atom" rel="self"/>
 <link href="http://notch8.com/"/>
 <updated>2013-07-22T06:54:32-07:00</updated>
 <id>http://notch8.com/</id>
 <author>
   <name>Rob Kaufman</name>
   <email>rob@notch8.com</email>
 </author>

 
 <entry>
   <title>EC2pocolyps Wrapup</title>
   <link href="http://notch8.com//2011/04/26/ec2pocolyps-wrapup.html"/>
   <updated>2011-04-26T00:00:00-07:00</updated>
   <id>http://notch8.com//2011/04/26/ec2pocolyps-wrapup</id>
   <content type="html">&lt;h1 id='what_happened'&gt;What happened&lt;/h1&gt;

&lt;p&gt;The long and short of it is that Amazon&amp;#8217;s cloud services failed and failed massively. Heroku, which many of our customers use, is built on top of Amazon, as are WebSolr, Exceptional and MongoHQ. These services where all heavily hurt by this unprecedented outage. Amazon is claiming that this outage is not a breach of the SLA, but the bottom line is that the outage was massive and the response was feeble. We where told that servers would be back on line in minutes when things first went down, and that they where working on the problem and it would be fixed &amp;#8220;soon&amp;#8221; basically every hour there after. Heroku on the other hand handled the outage as best they could, updating status.heroku.com quickly and accurately while being clear what promises they were and were not making.&lt;/p&gt;

&lt;h1 id='impact_on_notch8'&gt;Impact on Notch8&lt;/h1&gt;

&lt;p&gt;First off, I&amp;#8217;m very proud to say that none of Notch8&amp;#8217;s high availability customers experienced down time because of this outage. High availability is a trade off. It is, after all, expensive to maintain 2 full sets of architecture by two separate providers and two separate locations. We work hard to make things rock solid, and even with this major outage we were able to weather the storm without having to frantically scramble about. An event of this scale did mean degraded performance for some, though once we disabled the Exceptional javascript load (thanks to Jed Sundwall for noticing that) the &amp;#8220;backup&amp;#8221; servers where able to handle the load we asked of them during the outage.&lt;/p&gt;

&lt;p&gt;For those not on a high availability maintenance plan, things were definitely rockier. In fact, they were unacceptably rockier. I want to define my quality expectations here and then talk about in what ways they were not met and what will happen going forward.&lt;/p&gt;

&lt;h2 id='maintenance_customers'&gt;Maintenance Customers:&lt;/h2&gt;

&lt;p&gt;Those who have Notch8 maintaining their code and servers but who do not need to pay the extra server and sys admin costs of high availability can expect occasional down time. We define occasional down time as a few minutes here and there and maybe an hour or two in the event of something major. It doesn&amp;#8217;t mean down for two to three days. Yes, no data was lost even on servers whose EBS volumes did not come back at all. And yes, we were there to hold your hands and explain things as they happened. But it was still to long for me to be satisfied, and for that I apologize. We are going to be implementing smoother transitions to backup servers in the future which I&amp;#8217;ll layout below.&lt;/p&gt;

&lt;h2 id='develop_only_customers'&gt;Develop Only Customers:&lt;/h2&gt;

&lt;p&gt;We are not abandoning you if you opt out of a maintenance plan. We take a lot of pride in our work and are choosy about working on things we can be passionate about. It does mean that an outage like this will rely on you telling us that your services are down and authorizing a spend if you need to fail over to a backup stack. For you, an event like this may mean being down for two or three days until the underlying servers can be restored and it means that if we&amp;#8217;re scrambling with other customers that we can only put you in the queue and get to you as soon as possible. Not having a restoration plan in place will increase the time it costs when an incident happens. But we should always be able to move you over and get you back live. As I look at the land scape at the moment, were many of you only have the barest of backups so that I can sleep at night, I don&amp;#8217;t feel like that is necessarily the case. It should be, and I&amp;#8217;m sorry that I haven&amp;#8217;t made that more clear.&lt;/p&gt;

&lt;h1 id='what_is_going_to_change'&gt;What is Going to Change&lt;/h1&gt;

&lt;p&gt;In order to meet the standards we&amp;#8217;ve set we are going to be rolling out a few things:&lt;/p&gt;

&lt;p&gt;1) Better communication of site and service down time. We&amp;#8217;re going to be implementing a new dashboard page were those of you on maintenance plans will be able to get aggregated data as to what is going on.&lt;/p&gt;

&lt;p&gt;2) Accelerated deployment from backups for maintenance customers. We didn&amp;#8217;t start this soon enough and then hit some bumps in this work flow. We&amp;#8217;ll be ironing those out and testing the transition from the primary servers to their backup counter parts.&lt;/p&gt;

&lt;p&gt;3) A free backup audit for all customers (regardless of maintenance plan). We&amp;#8217;ll be putting together a document that shows your current stack, what is where and discussing your options in the even of a failure. This document will be pretty simple and hopefully really digestible. We&amp;#8217;ll also be showing you pricing should you decide you want to change to a different plan.&lt;/p&gt;

&lt;p&gt;These things are going to take a little time to get in place, so please bare with us.&lt;/p&gt;

&lt;h1 id='why_we_dont_think_we_should_move_stacks'&gt;Why We Don&amp;#8217;t Think We Should Move Stacks&lt;/h1&gt;

&lt;p&gt;Amazon and Heroku have both learned a ton from this outage. Heroku is for sure going to be making changes to address the gaps. It could happen to anyone and switching providers doesn&amp;#8217;t really protect us much anyway. Heroku has responded &lt;a href='http://status.heroku.com/incident/151'&gt;here&lt;/a&gt; by taking responsability and providing transparancy, which is exactly what I&amp;#8217;d hope for from a provider. Not only do we not gain anything by moving off Heroku, they are a responsive company, and will be better equipped to to handle an outage of this magnitude because of this outage. The benefits of Heroku are clear&amp;#8230; easy &amp;#8220;right sizing&amp;#8221; of servers and services. There is absolutely no easier way for us to deploy and scale your sites. I feel like with the addition of smoothing over our catastrophe response we can feel safe we&amp;#8217;re getting the best risk / cost trade off that we can.&lt;/p&gt;

&lt;h1 id='status_now'&gt;Status Now&lt;/h1&gt;

&lt;p&gt;All services are up and running. We&amp;#8217;ve verified everything we can think of, so if anything is found to be still down please let us know.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Adding Solr-Spatial-Light back to Sunspot</title>
   <link href="http://notch8.com//2011/02/22/sunspot-spatial.html"/>
   <updated>2011-02-22T00:00:00-08:00</updated>
   <id>http://notch8.com//2011/02/22/sunspot-spatial</id>
   <content type="html">&lt;p&gt;Geohashing is super cool, but it doesn&amp;#8217;t allow you to sort by distance (yet) and there are still some major issues when it comes to accuracy. Solr-spatial-light gives you accuracy and sortability, but it is more difficult to scale. To me (and my clients) &amp;#8220;works but takes more hardware&amp;#8221; trumps &amp;#8220;doesn&amp;#8217;t work but effecient&amp;#8221;. Word on the street is that the next release of Sunspot will once again support solr-spatial-light and that support will stay in until Geohashing becomes feature complete. In the meantime here are the steps to getting solr-spatial-light working with Sunspot 1.2&lt;/p&gt;

&lt;p&gt;In your searchable block add:&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='n'&gt;float&lt;/span&gt; &lt;span class='ss'&gt;:lat&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='ss'&gt;:as&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;lat&amp;#39;&lt;/span&gt;
&lt;span class='n'&gt;float&lt;/span&gt; &lt;span class='ss'&gt;:lng&lt;/span&gt;&lt;span class='p'&gt;,&lt;/span&gt; &lt;span class='ss'&gt;:as&lt;/span&gt; &lt;span class='o'&gt;=&amp;gt;&lt;/span&gt; &lt;span class='s1'&gt;&amp;#39;lng&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;These should be the latitude and longitude of your object. I&amp;#8217;m using &lt;a href='http://graticule.rubyforge.org/plugin.html'&gt;acts_as_geocodable&lt;/a&gt; to do my geocoding and storing the lat and lng as doubles in my db.&lt;/p&gt;

&lt;p&gt;In your search block you can filter using the adjust_solr_params method. I use a model to wrap my searches with a set of attr_accessors which get set by the form (that is where self.zipcode and self.with_in come from. The first is a postal code, the second is the radius number in miles.&lt;/p&gt;
&lt;div class='highlight'&gt;&lt;pre&gt;&lt;code class='ruby'&gt;&lt;span class='c1'&gt;# distance filters&lt;/span&gt;
&lt;span class='k'&gt;if&lt;/span&gt; &lt;span class='o'&gt;!&lt;/span&gt;&lt;span class='nb'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;zip_code&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;blank?&lt;/span&gt; &lt;span class='o'&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class='o'&gt;!&lt;/span&gt;&lt;span class='nb'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;with_in&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;blank?&lt;/span&gt; &lt;span class='o'&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class='n'&gt;code&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='no'&gt;Geocode&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;geocoder&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;locate&lt;/span&gt;&lt;span class='p'&gt;(&lt;/span&gt;&lt;span class='nb'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;zip_code&lt;/span&gt;&lt;span class='p'&gt;)&lt;/span&gt;
  &lt;span class='n'&gt;adjust_solr_params&lt;/span&gt; &lt;span class='k'&gt;do&lt;/span&gt; &lt;span class='o'&gt;|&lt;/span&gt;&lt;span class='n'&gt;params&lt;/span&gt;&lt;span class='o'&gt;|&lt;/span&gt;
    &lt;span class='n'&gt;params&lt;/span&gt;&lt;span class='o'&gt;[&lt;/span&gt;&lt;span class='ss'&gt;:spatial&lt;/span&gt;&lt;span class='o'&gt;]&lt;/span&gt; &lt;span class='o'&gt;=&lt;/span&gt; &lt;span class='s2'&gt;&amp;quot;{!radius=&lt;/span&gt;&lt;span class='si'&gt;#{&lt;/span&gt;&lt;span class='nb'&gt;self&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;with_in&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;to_i&lt;/span&gt;&lt;span class='si'&gt;}&lt;/span&gt;&lt;span class='s2'&gt;.0 sort=true}&lt;/span&gt;&lt;span class='si'&gt;#{&lt;/span&gt;&lt;span class='n'&gt;code&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;latitude&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;to_f&lt;/span&gt;&lt;span class='si'&gt;}&lt;/span&gt;&lt;span class='s2'&gt;,&lt;/span&gt;&lt;span class='si'&gt;#{&lt;/span&gt;&lt;span class='n'&gt;code&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;longitude&lt;/span&gt;&lt;span class='o'&gt;.&lt;/span&gt;&lt;span class='n'&gt;to_f&lt;/span&gt;&lt;span class='si'&gt;}&lt;/span&gt;&lt;span class='s2'&gt;&amp;quot;&lt;/span&gt;
    &lt;span class='c1'&gt;# spatial={!radius=10.0 sort=true}lat:40.0,lng:-70.0&lt;/span&gt;
  &lt;span class='k'&gt;end&lt;/span&gt;
&lt;span class='k'&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that&amp;#8217;s it. You now have solr-spatial-light support which is still supported by &lt;a href='http://websolr.com'&gt;WebSolr&lt;/a&gt; and is running beautifully in my production environment.&lt;/p&gt;</content>
 </entry>
 
 <entry>
   <title>Counting Retweets - Or Why Is This Hard</title>
   <link href="http://notch8.com//2010/06/21/twitter-search.html"/>
   <updated>2010-06-21T00:00:00-07:00</updated>
   <id>http://notch8.com//2010/06/21/twitter-search</id>
   <content type="html">&lt;body&gt;
  &lt;p&gt;
    Counting Retweets - Or Why Is This Hard
  &lt;/p&gt;
  &lt;p&gt;One of the primary goals of Measured Voice is to provide excellent statistics on each tweet sent by our users, including an accurate count of retweets. Counting retweets is a supremely difficult challenge. Here's whyâ€¦&lt;/p&gt;
  &lt;p&gt;The first challenge to counting retweets is defining what a retweet really is. Is it Twitter's official retweet? Is it any tweet that contains the exact same text as the original tweet? Or should it include the same link or only some part of the text? Do retweets need to include RT @username or /via @username or other attributions? It gets pretty confusing pretty fast.&lt;/p&gt;
  &lt;p&gt;Our plan for Measured Voice is to track both &quot;official&quot; and &quot;traditional&quot; retweets. We'd ultimately like to keep track of them separately, but limitations of Twitter's API make that impossible for now. For a long time, we'd been counting retweets using this algorithm:&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Search for the username of the Twitter account whose retweets we're trying to count (we'll just call this &quot;the account&quot; from here on)&lt;/li&gt;
    &lt;li&gt;Go through each result and check if we've seen it before&lt;/li&gt;
    &lt;li&gt;If we haven't seen it before, we check to see if it contains a matching short URL or the first six words of a message sent from the account&lt;/li&gt;
    &lt;li&gt;If it matches, we increment the retweet count&lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;We liked this approach because it&amp;mdash;in theory&amp;mdash;allowed us to count a lot of retweets from a lot of messages by performing only one simple search query for the username, saving us from expending API calls to count retweets by searching for short URLs for text from each individual message.&lt;/p&gt;
  &lt;p&gt;So yeah, in theory this should work fine, but it doesn't because Twitter search is a slippery beast. We've learned that when you search for a username, the results do not include all mentions of that username. We know that Twitter search only goes back about 7 days, but we're saying that the search results appear to randomly omit mentions of the username within that 7-day window.&lt;/p&gt;
  &lt;p&gt;For example, take the following tweet:&lt;/p&gt;
  &lt;style&gt;
    .bbpBox{background:#003366;padding:20px;}p.bbpTweet{background:#fff;padding:10px 12px 10px 12px;margin:0;min-height:48px;color:#000;font-size:18px !important;line-height:22px;-moz-border-radius:5px;-webkit-border-radius:5px}p.bbpTweet span.metadata{display:block;width:100%;clear:both;margin-top:8px;padding-top:12px;height:40px;border-top:1px solid #fff;border-top:1px solid #e6e6e6}p.bbpTweet span.metadata span.author{line-height:19px}p.bbpTweet span.metadata span.author img{float:left;margin:0 7px 0 0px;width:38px;height:38px}p.bbpTweet a:hover{text-decoration:underline}p.bbpTweet span.timestamp{font-size:12px;display:block}
  &lt;/style&gt;
  &lt;div class='bbpBox'&gt;
    &lt;p class='bbpTweet'&gt;
      There are 19 million new sexually transmitted infections each year. Most have no symptoms. Get yourself tested:
      &lt;a href='http://j.mp/9ikwGM' rel='nofollow'&gt;http://j.mp/9ikwGM&lt;/a&gt;
      &lt;span class='timestamp'&gt;
        &lt;a href='http://twitter.com/USAgov/status/11768661767' title='Wed Apr 07 18:02:44 +0000 2010'&gt;11:02 AM Apr 7th&lt;/a&gt;
        via
        &lt;a href='http://measuredvoice.com/' rel='nofollow'&gt;Measured Voice&lt;/a&gt;
      &lt;/span&gt;
      &lt;span class='metadata'&gt;
        &lt;span class='author'&gt;
          &lt;a href='http://twitter.com/USAgov'&gt;
            &lt;img src='http://a1.twimg.com/profile_images/52561878/usa_twitter_normal.gif'&gt;
          &lt;/a&gt;
          &lt;strong&gt;
            &lt;a href='http://twitter.com/USAgov'&gt;USA.gov&lt;/a&gt;
          &lt;/strong&gt;
          &lt;br&gt;USAgov
        &lt;/span&gt;
      &lt;/span&gt;
    &lt;/p&gt;
  &lt;/div&gt;
  &lt;!-- end of tweet --&gt;
  &lt;p&gt;
    Using the method described above, we'd
    &lt;a href='http://search.twitter.com/search?q=@usagov' title='@usagov - Twitter Search'&gt;search twitter for @USAgov&lt;/a&gt;
    and look for instances of
    &lt;a href='http://go.usa.gov/iUU' title='StopFraud.gov - Financial Fraud Enforcement Task Force'&gt;http://j.mp/9ikwGM&lt;/a&gt;
    within the results.
  &lt;/p&gt;
  Here is what we'd get:
  &lt;a href='/images/twitter-recounting-step-1-large.png'&gt;
    &lt;img src='/images/twitter-recounting-step-1-400.png'&gt;
  &lt;/a&gt;
  &lt;br&gt;
  &lt;span style='font-size: 80%;'&gt;Click to see fullsize&lt;/span&gt;
  &lt;p&gt;We get 6 messages that we would count (the two in red use a different link than the original, so we'd miss them). However, if we search for the link itself, we get this:&lt;/p&gt;
  &lt;a href='/images/twitter-recounting-step-2-large.png'&gt;
    &lt;img src='/images/twitter-recounting-step-2-400.png'&gt;
  &lt;/a&gt;
  &lt;br&gt;
  &lt;span style='font-size: 80%;'&gt;Click to see fullsize&lt;/span&gt;
  &lt;p&gt;
    Here we see the 6 results from the first query and 4 more! Those 4 in yellow
    &lt;strong&gt;never&lt;/strong&gt;
    show up in the @USAgov search. Where did they go? No one knows.
  &lt;/p&gt;
  &lt;p&gt;
    In fact, when we survey the various services that track retweets, this is what we find when looking for retweets of the above tweet:
  &lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;
      Gina Trapani's
      &lt;a href='http://thinktankapp.com/' title='ThinkTank - Ask your friends'&gt;Think Tank&lt;/a&gt;:
      5
    &lt;/li&gt;
    &lt;li&gt;Old Measured Voice username search method: 6&lt;/li&gt;
    &lt;li&gt;New Measured Voice URL search method: 10&lt;/li&gt;
    &lt;li&gt;OtterAPI: 10 (but a different 10!)&lt;/li&gt;
    &lt;li&gt;Retweet.net: 0&lt;/li&gt;
    &lt;li&gt;TweetMeme: 12 (counts original tweet)&lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;Why is this so hard? Why can't we go &quot;http://api.twitter.com/1/statuses/id/retweet_count&quot; and just get the number? Twitter makes it pretty clear that they have a hard time keeping up with API demand, so why are they asking us to pull each and every result over and over and manually count them? There are messages on the Google Group as far back as 2008 saying they are working on this, but we're curious how others are solving this problem.&lt;/p&gt;
  &lt;p&gt;
    Are we missing something obvious?
  &lt;/p&gt;
&lt;/body&gt;
&lt;div id='disqus_thread'&gt;&lt;/div&gt;
&lt;script&gt;
  var disqus_url = &quot;http://notch8.com//2010/06/21/twitter-search.html&quot;;
  var disqus_title =&quot;Counting Retweets - Or Why Is This Hard&quot;;
  var disqus_identifier = &quot;/2010/06/21/twitter-search.html&quot;;
  var disqus_developer = 1;
  (function() {
   var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
   dsq.src = 'http://notch8.disqus.com/embed.js';
   (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
    %noscript
  Please enable JavaScript to view the
  %a{:href =&gt; &quot;http://disqus.com/?ref_noscript=notch8&quot;} comments powered by Disqus.
&lt;/script&gt;
&lt;a class='dsq-brlink' href='http://disqus.com'&gt;
  blog comments powered by
  &lt;span class='logo-disqus'&gt;Disqus&lt;/span&gt;
&lt;/a&gt;
</content>
 </entry>
 
 
</feed>